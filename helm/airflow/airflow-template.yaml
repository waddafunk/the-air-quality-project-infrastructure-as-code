---
# Source: airflow/templates/rbac/airflow-serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: airflow-service-account
  labels:
    app: airflow
    chart: airflow-8.9.0
    release: airflow-cluster
    heritage: Helm
---
# Source: airflow/charts/redis/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: airflow-cluster-redis
  labels:
    app: redis
    chart: redis-10.5.7
    release: "airflow-cluster"
    heritage: "Helm"
type: Opaque
data:
  redis-password: "YWlyZmxvdw=="
---
# Source: airflow/templates/config/secret-config-envs.yaml
apiVersion: v1
kind: Secret
metadata:
  name: airflow-cluster-config-envs
  labels:
    app: airflow
    chart: airflow-8.9.0
    release: airflow-cluster
    heritage: Helm
## we must use `data` rather than `stringData` (see: https://github.com/helm/helm/issues/10010)
data:
  ## ================
  ## Linux Configs
  ## ================
  TZ: "RXRjL1VUQw=="

  ## ================
  ## Database Configs
  ## ================
  ## database host/port
  DATABASE_HOST: "YWlyZmxvdy1jbHVzdGVyLXBnYm91bmNlci5haXJmbG93LnN2Yy5jbHVzdGVyLmxvY2Fs"
  DATABASE_PORT: "NjQzMg=="

  ## database configs
  DATABASE_DB: "YWlyZmxvdw=="
  DATABASE_PROPERTIES: "P3NzbG1vZGU9cmVxdWlyZQ=="

  ## database credentials (from plain-text helm values)
  DATABASE_USER: "JHtQT1NUR1JFU19VU0VSfQ=="

  ## bash command which echos the URL encoded value of $DATABASE_USER
  DATABASE_USER_CMD: "ZWNobyAiJHtEQVRBQkFTRV9VU0VSfSIgfCBweXRob24zIC1jICJpbXBvcnQgdXJsbGliLnBhcnNlOyBlbmNvZGVkX3VzZXIgPSB1cmxsaWIucGFyc2UucXVvdGUoaW5wdXQoKSk7IHByaW50KGVuY29kZWRfdXNlciki"

  ## bash command which echos the URL encoded value of $DATABASE_PASSWORD
  DATABASE_PASSWORD_CMD: "ZWNobyAiJHtEQVRBQkFTRV9QQVNTV09SRH0iIHwgcHl0aG9uMyAtYyAiaW1wb3J0IHVybGxpYi5wYXJzZTsgZW5jb2RlZF9wYXNzID0gdXJsbGliLnBhcnNlLnF1b3RlKGlucHV0KCkpOyBwcmludChlbmNvZGVkX3Bhc3MpIg=="

  ## bash command which echos the DB connection string in SQLAlchemy format
  DATABASE_SQLALCHEMY_CMD: "ZWNobyAtbiAicG9zdGdyZXNxbCtwc3ljb3BnMjovLyQoZXZhbCAkREFUQUJBU0VfVVNFUl9DTUQpOiQoZXZhbCAkREFUQUJBU0VfUEFTU1dPUkRfQ01EKUAke0RBVEFCQVNFX0hPU1R9OiR7REFUQUJBU0VfUE9SVH0vJHtEQVRBQkFTRV9EQn0ke0RBVEFCQVNFX1BST1BFUlRJRVN9Ig=="

  ## bash command which echos the DB connection string in Celery result_backend format
  DATABASE_CELERY_CMD: "ZWNobyAtbiAiZGIrcG9zdGdyZXNxbDovLyQoZXZhbCAkREFUQUJBU0VfVVNFUl9DTUQpOiQoZXZhbCAkREFUQUJBU0VfUEFTU1dPUkRfQ01EKUAke0RBVEFCQVNFX0hPU1R9OiR7REFUQUJBU0VfUE9SVH0vJHtEQVRBQkFTRV9EQn0ke0RBVEFCQVNFX1BST1BFUlRJRVN9Ig=="
  ## bash command which echos the DB connection string in `psql` cli format
  ## NOTE: uses `127.0.0.1` as the host because this is only used in the pgbouncer liveness probe
  ##       and minikube does not allow pods to access their own `cluster.local` service so would otherwise fail
  DATABASE_PSQL_CMD: "ZWNobyAtbiAicG9zdGdyZXNxbDovLyQoZXZhbCAkREFUQUJBU0VfVVNFUl9DTUQpOiQoZXZhbCAkREFUQUJBU0VfUEFTU1dPUkRfQ01EKUAxMjcuMC4wLjE6JHtEQVRBQkFTRV9QT1JUfS8ke0RBVEFCQVNFX0RCfSR7REFUQUJBU0VfUFJPUEVSVElFU30i"

  ## ================
  ## Redis Configs
  ## ================
  ## connection string components
  REDIS_HOST: "YWlyZmxvdy1jbHVzdGVyLXJlZGlzLW1hc3Rlci5haXJmbG93LnN2Yy5jbHVzdGVyLmxvY2Fs"
  REDIS_PORT: "NjM3OQ=="
  REDIS_DBNUM: "MQ=="

  ## bash command which echos the URL encoded value of $REDIS_PASSWORD
  ## NOTE: if $REDIS_PASSWORD is non-empty, prints `:${REDIS_PASSWORD}@`, else ``
  REDIS_PASSWORD_CMD: "ZWNobyAiJHtSRURJU19QQVNTV09SRH0iIHwgcHl0aG9uMyAtYyAiaW1wb3J0IHVybGxpYi5wYXJzZTsgZW5jb2RlZF9wYXNzID0gdXJsbGliLnBhcnNlLnF1b3RlKGlucHV0KCkpOyBwcmludChmXCI6e2VuY29kZWRfcGFzc31AXCIpIGlmIGxlbihlbmNvZGVkX3Bhc3MpID4gMCBlbHNlIE5vbmUi"

  ## bash command which echos the Redis connection string
  REDIS_CONNECTION_CMD: "ZWNobyAtbiAicmVkaXM6Ly8kKGV2YWwgJFJFRElTX1BBU1NXT1JEX0NNRCkke1JFRElTX0hPU1R9OiR7UkVESVNfUE9SVH0vJHtSRURJU19EQk5VTX0ke1JFRElTX1BST1BFUlRJRVN9Ig=="

  ## ================
  ## Airflow Configs (General)
  ## ================
  AIRFLOW__CORE__DAGS_FOLDER: "L29wdC9haXJmbG93L2RhZ3M="
  AIRFLOW__CORE__EXECUTOR: "Q2VsZXJ5RXhlY3V0b3I="
  AIRFLOW__CORE__FERNET_KEY: "N1Q1MTJVWFNTbUJPa3BXaW1GSElWYjhqSzZsZm1TQXZ4NG1PNkFyZWhuYz0="
  AIRFLOW__WEBSERVER__SECRET_KEY: "VEhJUyBJUyBVTlNBRkUh"
  AIRFLOW__WEBSERVER__WEB_SERVER_PORT: "ODA4MA=="
  AIRFLOW__CELERY__FLOWER_PORT: "NTU1NQ=="

  ## ================
  ## Airflow Configs (Database)
  ## ================
  AIRFLOW__CORE__SQL_ALCHEMY_CONN_CMD: "YmFzaCAtYyAnZXZhbCAiJERBVEFCQVNFX1NRTEFMQ0hFTVlfQ01EIic="
  ## `core.sql_alchemy_conn` moved to `database.sql_alchemy_conn` in airflow 2.3.0
  AIRFLOW__DATABASE__SQL_ALCHEMY_CONN_CMD: "YmFzaCAtYyAnZXZhbCAiJERBVEFCQVNFX1NRTEFMQ0hFTVlfQ01EIic="

  ## ================
  ## Airflow Configs (Triggerer)
  ## ================
  AIRFLOW__TRIGGERER__DEFAULT_CAPACITY: "MTAwMA=="

  ## ================
  ## Airflow Configs (Logging)
  ## ================
  AIRFLOW__LOGGING__BASE_LOG_FOLDER: "L29wdC9haXJmbG93L2xvZ3M="
  AIRFLOW__LOGGING__DAG_PROCESSOR_MANAGER_LOG_LOCATION: "L29wdC9haXJmbG93L2xvZ3MvZGFnX3Byb2Nlc3Nvcl9tYW5hZ2VyL2RhZ19wcm9jZXNzb3JfbWFuYWdlci5sb2c="
  AIRFLOW__SCHEDULER__CHILD_PROCESS_LOG_DIRECTORY: "L29wdC9haXJmbG93L2xvZ3Mvc2NoZWR1bGVy"

  ## ================
  ## Airflow Configs (Celery)
  ## ================
  AIRFLOW__LOGGING__WORKER_LOG_SERVER_PORT: "ODc5Mw=="
  # `logging.worker_log_server_port` replaced `celery.worker_log_server_port` in airflow 2.2.0
  AIRFLOW__CELERY__WORKER_LOG_SERVER_PORT: "ODc5Mw=="
  AIRFLOW__CELERY__BROKER_URL_CMD: "YmFzaCAtYyAnZXZhbCAiJFJFRElTX0NPTk5FQ1RJT05fQ01EIic="
  AIRFLOW__CELERY__RESULT_BACKEND_CMD: "YmFzaCAtYyAnZXZhbCAiJERBVEFCQVNFX0NFTEVSWV9DTUQiJw=="

  ## ================
  ## Airflow Configs (Kubernetes)
  ## ================

  ## ================
  ## User Configs
  ## ================
---
# Source: airflow/templates/config/secret-webserver-config.yaml
apiVersion: v1
kind: Secret
metadata:
  name: airflow-cluster-webserver-config
  labels:
    app: airflow
    chart: airflow-8.9.0
    release: airflow-cluster
    heritage: Helm
data:
  webserver_config.py: "ZnJvbSBmbGFza19hcHBidWlsZGVyLnNlY3VyaXR5Lm1hbmFnZXIgaW1wb3J0IEFVVEhfREIKCiMgdXNlIGVtYmVkZGVkIERCIGZvciBhdXRoCkFVVEhfVFlQRSA9IEFVVEhfREIK"
---
# Source: airflow/templates/db-migrations/db-migrations-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: airflow-cluster-db-migrations
  labels:
    app: airflow
    component: db-migrations
    chart: airflow-8.9.0
    release: airflow-cluster
    heritage: Helm
data:
  db_migrations.py: "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"
---
# Source: airflow/templates/pgbouncer/pgbouncer-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: airflow-cluster-pgbouncer
  labels:
    app: airflow
    component: pgbouncer
    chart: airflow-8.9.0
    release: airflow-cluster
    heritage: Helm
data:
  pgbouncer.ini: "CltkYXRhYmFzZXNdCiogPSBob3N0PSR7UE9TVEdSRVNfSE9TVH0gcG9ydD01NDMyCgpbcGdib3VuY2VyXQpwb29sX21vZGUgPSB0cmFuc2FjdGlvbgptYXhfY2xpZW50X2Nvbm4gPSAxMDAwCmRlZmF1bHRfcG9vbF9zaXplID0gIDIwCmlnbm9yZV9zdGFydHVwX3BhcmFtZXRlcnMgPSBleHRyYV9mbG9hdF9kaWdpdHMKCmxpc3Rlbl9wb3J0ID0gNjQzMgpsaXN0ZW5fYWRkciA9ICoKCmF1dGhfdHlwZSA9IG1kNQphdXRoX2ZpbGUgPSAvaG9tZS9wZ2JvdW5jZXIvdXNlcnMudHh0Cgpsb2dfZGlzY29ubmVjdGlvbnMgPSAwCmxvZ19jb25uZWN0aW9ucyA9IDAKCiMgbG9ja3Mgd2lsbCBuZXZlciBiZSByZWxlYXNlZCB3aGVuIGBwb29sX21vZGU9dHJhbnNhY3Rpb25gIChhaXJmbG93IGluaXRkYi91cGdyYWRlZGIgc2NyaXB0cyBjcmVhdGUgbG9ja3MpCnNlcnZlcl9yZXNldF9xdWVyeSA9IFNFTEVDVCBwZ19hZHZpc29yeV91bmxvY2tfYWxsKCkKc2VydmVyX3Jlc2V0X3F1ZXJ5X2Fsd2F5cyA9IDEKCiMjIENMSUVOVCBUTFMgU0VUVElOR1MgIyMKY2xpZW50X3Rsc19zc2xtb2RlID0gcHJlZmVyCmNsaWVudF90bHNfY2lwaGVycyA9IG5vcm1hbApjbGllbnRfdGxzX2tleV9maWxlID0gL2hvbWUvcGdib3VuY2VyL2dlbmVyYXRlZC1jZXJ0cy9jbGllbnQua2V5CmNsaWVudF90bHNfY2VydF9maWxlID0gL2hvbWUvcGdib3VuY2VyL2dlbmVyYXRlZC1jZXJ0cy9jbGllbnQuY3J0CgojIyBTRVJWRVIgVExTIFNFVFRJTkdTICMjCnNlcnZlcl90bHNfc3NsbW9kZSA9IHByZWZlcgpzZXJ2ZXJfdGxzX2NpcGhlcnMgPSBub3JtYWw="
  gen_auth_file.sh: "CiMhL2Jpbi9zaCAtZQoKIyBERVNDUklQVElPTjoKIyAtIHVwZGF0ZXMgdGhlIHBnYm91bmNlciBgYXV0aF9maWxlYCBmcm9tIGVudmlyb25tZW50IHZhcmlhYmxlcwojIC0gY2FsbGVkIGluIG1haW4gcGdib3VuY2VyIGNvbnRhaW5lciBzdGFydC1jb21tYW5kIHNvIHRoYXQgYGF1dGhfZmlsZWAgaXMgdXBkYXRlZCBlYWNoIHJlc3RhcnQsCiMgICBmb3IgZXhhbXBsZSwgd2hlbiB0aGUgbGl2ZW5lc3NQcm9iZSBmYWlscyBkdWUgdG8gYSBEQVRBQkFTRV9QQVNTV09SRCBzZWNyZXQgdXBkYXRlCgojIHZhcmlhYmxlcyB0byBpbmNyZWFzZSBjbGFyaXR5IG9mIHBhdHRlcm4gbWF0Y2hpbmcKT05FX1FVT1RFPSciJwpUV09fUVVPVEU9JyIiJwoKIyBwZ2JvdW5jZXIgcmVxdWlyZXMgYCJgIHRvIGJlIGVzY2FwZWQgYXMgYCIiYApFU0NBUEVEX0RBVEFCQVNFX1VTRVI9IiR7REFUQUJBU0VfVVNFUi8kT05FX1FVT1RFLyRUV09fUVVPVEV9IgpFU0NBUEVEX0RBVEFCQVNFX1BBU1NXT1JEPSIke0RBVEFCQVNFX1BBU1NXT1JELyRPTkVfUVVPVEUvJFRXT19RVU9URX0iCgojIHBnYm91bmNlciByZXF1aXJlcyBhdXRoX2ZpbGUgaW4gZm9ybWF0IGAibXktdXNlcm5hbWUiICJteS1wYXNzd29yZCJgCmVjaG8gXCIkRVNDQVBFRF9EQVRBQkFTRV9VU0VSXCIgXCIkRVNDQVBFRF9EQVRBQkFTRV9QQVNTV09SRFwiID4gL2hvbWUvcGdib3VuY2VyL3VzZXJzLnR4dAplY2hvICJTdWNjZXNzZnVsbHkgZ2VuZXJhdGVkIGF1dGhfZmlsZTogL2hvbWUvcGdib3VuY2VyL3VzZXJzLnR4dCI="
  gen_self_signed_cert.sh: "CiMhL2Jpbi9zaCAtZQoKQ0VSVF9ESVI9Ii9ob21lL3BnYm91bmNlci9nZW5lcmF0ZWQtY2VydHMiCktFWV9GSUxFPSIkQ0VSVF9ESVIvY2xpZW50LmtleSIKQ0VSVF9GSUxFPSIkQ0VSVF9ESVIvY2xpZW50LmNydCIKCiMgY3JlYXRlIHRoZSBkaXJlY3RvcnkgZm9yIHRoZSBzZWxmLXNpZ25lZCBjZXJ0aWZpY2F0ZQpta2RpciAtcCAiJENFUlRfRElSIgoKIyB2YXJpYWJsZXMgZm9yIGNlcnRpZmljYXRlIGdlbmVyYXRpb24KQ09NTU9OX05BTUU9ImxvY2FsaG9zdCIKREFZU19WQUxJRD0zNjUKCiMgZ2VuZXJhdGUgdGhlIHNlbGYtc2lnbmVkIGNlcnRpZmljYXRlIGFuZCBhIHByaXZhdGUga2V5Cm9wZW5zc2wgcmVxIC14NTA5IFwKICAtbmV3a2V5IHJzYTo0MDk2IFwKICAta2V5b3V0ICIkS0VZX0ZJTEUiIFwKICAtb3V0ICIkQ0VSVF9GSUxFIiBcCiAgLWRheXMgIiREQVlTX1ZBTElEIiBcCiAgLXN1YmogIi9DTj0kQ09NTU9OX05BTUUiIFwKICAtbm9kZXMKCiMgc2V0IHBlcm1pc3Npb25zIGZvciB0aGUgcHJpdmF0ZSBrZXkgZmlsZQpjaG1vZCA2MDAgIiRLRVlfRklMRSIKCmVjaG8gIlN1Y2Nlc3NmdWxseSBnZW5lcmF0ZWQgc2VsZi1zaWduZWQgY2VydGlmaWNhdGU6ICRDRVJUX0ZJTEUiCmVjaG8gIlN1Y2Nlc3NmdWxseSBnZW5lcmF0ZWQgc2VsZi1zaWduZWQgY2VydGlmaWNhdGUga2V5OiAkS0VZX0ZJTEUi"
---
# Source: airflow/templates/sync/sync-users-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: airflow-cluster-sync-users
  labels:
    app: airflow
    component: sync-users
    chart: airflow-8.9.0
    release: airflow-cluster
    heritage: Helm
data:
  sync_users.py: "
############################
#### BEGIN: GLOBAL CODE ####
############################
####################
## Global Imports ##
####################
import logging
import os
import time
from string import Template
from typing import List, Dict, Optional


####################
## Global Configs ##
####################
# the path which Secret/ConfigMap are mounted to
CONF__TEMPLATES_PATH = "/mnt/templates"

# how frequently to check for Secret/ConfigMap updates
CONF__TEMPLATES_SYNC_INTERVAL = 10

# how frequently to re-sync objects (Connections, Pools, Users, Variables)
CONF__OBJECTS_SYNC_INTERVAL = 60


######################
## Global Functions ##
######################
def string_substitution(raw_string: Optional[str], substitution_map: Dict[str, str]) -> str:
    """
    Apply bash-like substitutions to a raw string.

    Example:
    - string_substitution("Hello!", None) -> "Hello!"
    - string_substitution("Hello ${NAME}!", {"NAME": "Airflow"}) -> "Hello Airflow!"
    """
    if raw_string and len(substitution_map) > 0:
        tpl = Template(raw_string)
        return tpl.safe_substitute(substitution_map)
    else:
        return raw_string


def template_mtime(template_name: str) -> float:
    """
    Return the modification-time of the file storing `template_name`
    """
    file_path = f"{CONF__TEMPLATES_PATH}/{template_name}"
    return os.stat(file_path).st_mtime


def template_value(template_name: str) -> str:
    """
    Return the contents of the file storing `template_name`
    """
    file_path = f"{CONF__TEMPLATES_PATH}/{template_name}"
    with open(file_path, "r") as f:
        return f.read()


def refresh_template_cache(template_names: List[str],
                           template_mtime_cache: Dict[str, float],
                           template_value_cache: Dict[str, str]) -> List[str]:
    """
    Refresh the provided dictionary caches of template values & mtimes.

    :param template_names: the names of all templates to refresh
    :param template_mtime_cache: the dictionary cache of template file modification-times
    :param template_value_cache: the dictionary cache of template values
    :return: the names of templates which changed
    """
    changed_templates = []
    for template_name in template_names:
        old_mtime = template_mtime_cache.get(template_name, None)
        new_mtime = template_mtime(template_name)
        # first, check if the files were modified
        if old_mtime != new_mtime:
            old_value = template_value_cache.get(template_name, None)
            new_value = template_value(template_name)
            # second, check if the value actually changed
            if old_value != new_value:
                template_value_cache[template_name] = new_value
                changed_templates += [template_name]
            template_mtime_cache[template_name] = new_mtime
    return changed_templates


def main(sync_forever: bool):
    # initial sync of template cache
    refresh_template_cache(
        template_names=VAR__TEMPLATE_NAMES,
        template_mtime_cache=VAR__TEMPLATE_MTIME_CACHE,
        template_value_cache=VAR__TEMPLATE_VALUE_CACHE
    )

    # initial sync of objects into Airflow DB
    sync_with_airflow()

    if sync_forever:
        # define variables used to track how long since last refresh/sync
        templates_sync_epoch = time.time()
        objects_sync_epoch = time.time()

        # main loop
        while True:
            # monitor for template secret/configmap updates
            if (time.time() - templates_sync_epoch) > CONF__TEMPLATES_SYNC_INTERVAL:
                logging.debug(f"template sync interval reached, re-syncing all templates...")
                changed_templates = refresh_template_cache(
                    template_names=VAR__TEMPLATE_NAMES,
                    template_mtime_cache=VAR__TEMPLATE_MTIME_CACHE,
                    template_value_cache=VAR__TEMPLATE_VALUE_CACHE
                )
                templates_sync_epoch = time.time()
                if changed_templates:
                    logging.info(f"template values have changed: [{','.join(changed_templates)}]")
                    sync_with_airflow()
                    objects_sync_epoch = time.time()

            # monitor for external changes to objects (like from UI)
            if (time.time() - objects_sync_epoch) > CONF__OBJECTS_SYNC_INTERVAL:
                logging.debug(f"sync interval reached, re-syncing all objects...")
                sync_with_airflow()
                objects_sync_epoch = time.time()

            # ensure we dont loop too fast
            time.sleep(0.5)
##########################
#### END: GLOBAL CODE ####
##########################


#############
## Imports ##
#############
import sys
from werkzeug.security import check_password_hash, generate_password_hash
import airflow.www.app as www_app
flask_app = www_app.create_app()
flask_appbuilder = flask_app.appbuilder

# we want type hints, but airflow keeps moving the `User` and `Role` models around
#                                  (╯°□°)╯︵ ┻━┻
from typing import Any
User = Any
Role = Any

#############
## Classes ##
#############
class UserWrapper(object):
    def __init__(
            self,
            username: str,
            first_name: Optional[str] = None,
            last_name: Optional[str] = None,
            email: Optional[str] = None,
            roles: Optional[List[str]] = None,
            password: Optional[str] = None
    ):
        self.username = username
        self._first_name = first_name
        self._last_name = last_name
        self._email = email
        self.roles = roles
        self._password = password

    @property
    def first_name(self) -> str:
        return string_substitution(self._first_name, VAR__TEMPLATE_VALUE_CACHE)

    @property
    def last_name(self) -> str:
        return string_substitution(self._last_name, VAR__TEMPLATE_VALUE_CACHE)

    @property
    def email(self) -> str:
        return string_substitution(self._email, VAR__TEMPLATE_VALUE_CACHE)

    @property
    def password(self) -> str:
        return string_substitution(self._password, VAR__TEMPLATE_VALUE_CACHE)

    def as_dict(self) -> Dict[str, str]:
        return {
            "username": self.username,
            "first_name": self.first_name,
            "last_name": self.last_name,
            "email": self.email,
            "roles": [find_role(role_name=role_name) for role_name in self.roles],
            "password": self.password
        }


###############
## Variables ##
###############
VAR__TEMPLATE_NAMES = [
]
VAR__TEMPLATE_MTIME_CACHE = {}
VAR__TEMPLATE_VALUE_CACHE = {}
VAR__USER_WRAPPERS = {
  "admin": UserWrapper(
    username="admin",
    first_name="admin",
    last_name="admin",
    email="admin@example.com",
    roles=[        "Admin",
    ],
    password="admin",
  ),
}


###############
## Functions ##
###############
def find_role(role_name: str) -> Role:
    """
    Get the FAB Role model associated with a `role_name`.
    """
    found_role = flask_appbuilder.sm.find_role(role_name)
    if found_role:
        return found_role
    else:
        valid_roles = flask_appbuilder.sm.get_all_roles()
        logging.error(f"Failed to find role=`{role_name}`, valid roles are: {valid_roles}")
        sys.exit(1)


def compare_role_lists(role_list_1: List[Role], role_list_2: List[Role]) -> bool:
    """
    Check if two lists of FAB Roles contain the same roles (ignores duplicates and order).
    """
    name_set_1 = set(role.name for role in role_list_1)
    name_set_2 = set(role.name for role in role_list_2)
    return name_set_1 == name_set_2



def compare_users(user_dict: Dict, user_model: User) -> bool:
    """
    Check if user info (stored in dict) is identical to a FAB User model.
    """
    return (
            user_dict["username"] == user_model.username
            and user_dict["first_name"] == user_model.first_name
            and user_dict["last_name"] == user_model.last_name
            and user_dict["email"] == user_model.email
            and compare_role_lists(user_dict["roles"], user_model.roles)
            and check_password_hash(pwhash=user_model.password, password=user_dict["password"])
    )


def sync_user(user_wrapper: UserWrapper) -> None:
    """
    Sync the User defined by a provided UserWrapper into the FAB DB.
    """
    username = user_wrapper.username
    u_new = user_wrapper.as_dict()
    u_old = flask_appbuilder.sm.find_user(username=username)

    if not u_old:
        logging.info(f"User=`{username}` is missing, adding...")
        created_user = flask_appbuilder.sm.add_user(
            username=u_new["username"],
            first_name=u_new["first_name"],
            last_name=u_new["last_name"],
            email=u_new["email"],
            # in old versions of flask_appbuilder `add_user(role=` can only add exactly one role
            # (unchecked 0 index is safe because we require at least one role using helm values validation)
            role=u_new["roles"][0],
            password=u_new["password"]
        )
        if created_user:
            # add the full list of roles (we only added the first one above)
            created_user.roles = u_new["roles"]
            logging.info(f"User=`{username}` was successfully added.")
        else:
            logging.error(f"Failed to add User=`{username}`")
            sys.exit(1)
    else:
        if compare_users(u_new, u_old):
            pass
        else:
            logging.info(f"User=`{username}` exists but has changed, updating...")
            u_old.first_name = u_new["first_name"]
            u_old.last_name = u_new["last_name"]
            u_old.email = u_new["email"]
            u_old.roles = u_new["roles"]
            u_old.password = generate_password_hash(u_new["password"])
            # strange check for False is because update_user() returns None for success
            # but in future might return the User model
            if not (flask_appbuilder.sm.update_user(u_old) is False):
                logging.info(f"User=`{username}` was successfully updated.")
            else:
                logging.error(f"Failed to update User=`{username}`")
                sys.exit(1)


def sync_all_users(user_wrappers: Dict[str, UserWrapper]) -> None:
    """
    Sync all users in provided `user_wrappers`.
    """
    logging.info("BEGIN: airflow users sync")
    for user_wrapper in user_wrappers.values():
        sync_user(user_wrapper)
    logging.info("END: airflow users sync")

    # ensures than any SQLAlchemy sessions are closed (so we don't hold a connection to the database)
    flask_app.do_teardown_appcontext()


def sync_with_airflow() -> None:
    """
    Preform a sync of all objects with airflow (note, `sync_with_airflow()` is called in `main()` template).
    """
    sync_all_users(user_wrappers=VAR__USER_WRAPPERS)


##############
## Run Main ##
##############
main(sync_forever=True)"
---
# Source: airflow/charts/redis/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: airflow-cluster-redis
  labels:
    app: redis
    chart: redis-10.5.7
    heritage: Helm
    release: airflow-cluster
data:
  redis.conf: |-
    # User-supplied configuration:
    # Enable AOF https://redis.io/topics/persistence#append-only-file
    appendonly yes
    # Disable RDB persistence, AOF persistence already enabled.
    save ""
  master.conf: |-
    dir /data
    rename-command FLUSHDB ""
    rename-command FLUSHALL ""
  replica.conf: |-
    dir /data
    slave-read-only yes
    rename-command FLUSHDB ""
    rename-command FLUSHALL ""
---
# Source: airflow/charts/redis/templates/health-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: airflow-cluster-redis-health
  labels:
    app: redis
    chart: redis-10.5.7
    heritage: Helm
    release: airflow-cluster
data:
  ping_readiness_local.sh: |-
    response=$(
      timeout -s 9 $1 \
      redis-cli \
        -a $REDIS_PASSWORD --no-auth-warning \
        -h localhost \
        -p $REDIS_PORT \
        ping
    )
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
  ping_liveness_local.sh: |-
    response=$(
      timeout -s 9 $1 \
      redis-cli \
        -a $REDIS_PASSWORD --no-auth-warning \
        -h localhost \
        -p $REDIS_PORT \
        ping
    )
    if [ "$response" != "PONG" ] && [ "$response" != "LOADING Redis is loading the dataset in memory" ]; then
      echo "$response"
      exit 1
    fi
  ping_readiness_master.sh: |-
    response=$(
      timeout -s 9 $1 \
      redis-cli \
        -a $REDIS_MASTER_PASSWORD --no-auth-warning \
        -h $REDIS_MASTER_HOST \
        -p $REDIS_MASTER_PORT_NUMBER \
        ping
    )
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
  ping_liveness_master.sh: |-
    response=$(
      timeout -s 9 $1 \
      redis-cli \
        -a $REDIS_MASTER_PASSWORD --no-auth-warning \
        -h $REDIS_MASTER_HOST \
        -p $REDIS_MASTER_PORT_NUMBER \
        ping
    )
    if [ "$response" != "PONG" ] && [ "$response" != "LOADING Redis is loading the dataset in memory" ]; then
      echo "$response"
      exit 1
    fi
  ping_readiness_local_and_master.sh: |-
    script_dir="$(dirname "$0")"
    exit_status=0
    "$script_dir/ping_readiness_local.sh" $1 || exit_status=$?
    "$script_dir/ping_readiness_master.sh" $1 || exit_status=$?
    exit $exit_status
  ping_liveness_local_and_master.sh: |-
    script_dir="$(dirname "$0")"
    exit_status=0
    "$script_dir/ping_liveness_local.sh" $1 || exit_status=$?
    "$script_dir/ping_liveness_master.sh" $1 || exit_status=$?
    exit $exit_status
---
# Source: airflow/templates/pvc-dags.yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: airflow-cluster-dags
  labels:
    app: airflow
    chart: airflow-8.9.0
    release: airflow-cluster
    heritage: Helm
spec:
  accessModes:
    - "ReadOnlyMany"
  resources:
    requests:
      storage: "1Gi"
---
# Source: airflow/templates/pvc-logs.yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: airflow-cluster-logs
  labels:
    app: airflow
    chart: airflow-8.9.0
    release: airflow-cluster
    heritage: Helm
spec:
  accessModes:
    - "ReadWriteMany"
  resources:
    requests:
      storage: "5Gi"
---
# Source: airflow/templates/rbac/airflow-role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: airflow-cluster
  labels:
    app: airflow
    chart: airflow-8.9.0
    release: airflow-cluster
    heritage: Helm
rules:
- apiGroups:
  - ""
  resources:
  - events
  verbs:
  - "get"
  - "list"
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - "create"
  - "get"
  - "delete"
  - "list"
  - "patch"
  - "watch"
- apiGroups:
  - ""
  resources:
  - "pods/log"
  verbs:
  - "get"
  - "list"
- apiGroups:
  - ""
  resources:
  - "pods/exec"
  verbs:
  - "create"
  - "get"
---
# Source: airflow/templates/rbac/airflow-rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: airflow-cluster
  labels:
    app: airflow
    chart: airflow-8.9.0
    release: airflow-cluster
    heritage: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: airflow-cluster
subjects:
- kind: ServiceAccount
  name: airflow-service-account
  namespace: airflow
---
# Source: airflow/charts/redis/templates/headless-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: airflow-cluster-redis-headless
  labels:
    app: redis
    chart: redis-10.5.7
    release: airflow-cluster
    heritage: Helm
spec:
  type: ClusterIP
  clusterIP: None
  ports:
  - name: redis
    port: 6379
    targetPort: redis
  selector:
    app: redis
    release: airflow-cluster
---
# Source: airflow/charts/redis/templates/redis-master-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: airflow-cluster-redis-master
  labels:
    app: redis
    chart: redis-10.5.7
    release: airflow-cluster
    heritage: Helm
spec:
  type: ClusterIP
  ports:
  - name: redis
    port: 6379
    targetPort: redis
  selector:
    app: redis
    release: airflow-cluster
    role: master
---
# Source: airflow/templates/flower/flower-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: airflow-cluster-flower
  labels:
    app: airflow
    component: flower
    chart: airflow-8.9.0
    release: airflow-cluster
    heritage: Helm
spec:
  type: ClusterIP
  selector:
    app: airflow
    component: flower
    release: airflow-cluster
  ports:
    - name: flower
      ## NOTE: flower always uses http (only important for Istio users)
      appProtocol: http
      protocol: TCP
      port: 5555
      targetPort: 5555
---
# Source: airflow/templates/pgbouncer/pgbouncer-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: airflow-cluster-pgbouncer
  labels:
    app: airflow
    component: pgbouncer
    chart: airflow-8.9.0
    release: airflow-cluster
    heritage: Helm
spec:
  type: ClusterIP
  selector:
    app: airflow
    component: pgbouncer
    release: airflow-cluster
  ports:
    - name: pgbouncer
      ## NOTE: pgbouncer should be treated as opaque TCP (only important for Istio users)
      appProtocol: tcp
      protocol: TCP
      port: 6432
---
# Source: airflow/templates/webserver/webserver-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: airflow-cluster-web
  labels:
    app: airflow
    component: web
    chart: airflow-8.9.0
    release: airflow-cluster
    heritage: Helm
spec:
  type: ClusterIP
  selector:
    app: airflow
    component: web
    release: airflow-cluster
  sessionAffinity: None
  ports:
    - name: web
      appProtocol: "http"
      protocol: TCP
      port: 8080
      targetPort: 8080
---
# Source: airflow/templates/worker/worker-service.yaml
apiVersion: v1
## this Service gives stable DNS entries for workers, used by webserver for logs
kind: Service
metadata:
  name: airflow-cluster-worker
  labels:
    app: airflow
    component: worker
    chart: airflow-8.9.0
    release: airflow-cluster
    heritage: Helm
spec:
  ports:
    - name: worker
      ## NOTE: the worker logs port is always http (only important for Istio users)
      ##       https://github.com/apache/airflow/blob/2.9.0/airflow/utils/log/file_task_handler.py#L415
      appProtocol: http
      protocol: TCP
      port: 8793
  clusterIP: None
  selector:
    app: airflow
    component: worker
    release: airflow-cluster
---
# Source: airflow/templates/db-migrations/db-migrations-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: airflow-cluster-db-migrations
  labels:
    app: airflow
    component: db-migrations
    chart: airflow-8.9.0
    release: airflow-cluster
    heritage: Helm
spec:
  replicas: 1
  strategy:
    ## only 1 replica should run at a time
    type: Recreate
  selector:
    matchLabels:
      app: airflow
      component: db-migrations
      release: airflow-cluster
  template:
    metadata:
      annotations:
        checksum/secret-config-envs: c01fcf854921a69c2cd9e437e9d5e73c4e69d5cbad62a0c53bd258bcd62996a3
        checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
        checksum/db-migrations-script: 37898f38b90abd06081105d992362ec9e0d0015123b69e758e59031a9e6ddfc9
        cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      labels:
        app: airflow
        component: db-migrations
        release: airflow-cluster
    spec:
      restartPolicy: Always
      nodeSelector:
        {}
      topologySpreadConstraints:
        []
      affinity:
        {}
      tolerations:
        []
      securityContext:
        fsGroup: 0
      serviceAccountName: airflow-service-account
      initContainers:        
        - name: check-db  
          image: apache/airflow:2.8.4-python3.9
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          resources:
            {}
          envFrom:    
            - secretRef:
                name: airflow-cluster-config-envs
          env:    
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgres-secret
                  key: postgres-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-cluster-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:    
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec timeout 60s airflow db check"
          volumeMounts:    
            - name: dags-data
              mountPath: /opt/airflow/dags
              subPath: 
              readOnly: true
            - name: logs-data
              mountPath: /opt/airflow/logs
              subPath: 
      containers:
        - name: db-migrations          
          image: apache/airflow:2.8.4-python3.9
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          resources:
            {}
          envFrom:            
            - secretRef:
                name: airflow-cluster-config-envs
          env:            
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgres-secret
                  key: postgres-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-cluster-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:            
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "python"
            - "-u"
            - "/mnt/scripts/db_migrations.py"
          volumeMounts:            
            - name: dags-data
              mountPath: /opt/airflow/dags
              subPath: 
              readOnly: true
            - name: logs-data
              mountPath: /opt/airflow/logs
              subPath: 
            - name: scripts
              mountPath: /mnt/scripts
              readOnly: true
      volumes:        
        - name: dags-data
          persistentVolumeClaim:
            claimName: airflow-cluster-dags
        - name: logs-data
          persistentVolumeClaim:
            claimName: airflow-cluster-logs
        - name: scripts
          secret:
            secretName: airflow-cluster-db-migrations
---
# Source: airflow/templates/flower/flower-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: airflow-cluster-flower
  labels:
    app: airflow
    component: flower
    chart: airflow-8.9.0
    release: airflow-cluster
    heritage: Helm
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      ## multiple flower pods can safely run concurrently
      maxSurge: 25%
      maxUnavailable: 0
  selector:
    matchLabels:
      app: airflow
      component: flower
      release: airflow-cluster
  template:
    metadata:
      annotations:
        checksum/secret-config-envs: c01fcf854921a69c2cd9e437e9d5e73c4e69d5cbad62a0c53bd258bcd62996a3
        checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
        cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      labels:
        app: airflow
        component: flower
        release: airflow-cluster
    spec:
      restartPolicy: Always
      nodeSelector:
        {}
      topologySpreadConstraints:
        []
      affinity:
        {}
      tolerations:
        []
      securityContext:
        fsGroup: 0
      serviceAccountName: airflow-service-account
      initContainers:        
        - name: check-db  
          image: apache/airflow:2.8.4-python3.9
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          resources:
            {}
          envFrom:    
            - secretRef:
                name: airflow-cluster-config-envs
          env:    
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgres-secret
                  key: postgres-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-cluster-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:    
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec timeout 60s airflow db check"
          volumeMounts:    
            - name: dags-data
              mountPath: /opt/airflow/dags
              subPath: 
              readOnly: true
            - name: logs-data
              mountPath: /opt/airflow/logs
              subPath:         
        - name: wait-for-db-migrations  
          image: apache/airflow:2.8.4-python3.9
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          resources:
            {}
          envFrom:    
            - secretRef:
                name: airflow-cluster-config-envs
          env:    
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgres-secret
                  key: postgres-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-cluster-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:    
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec airflow db check-migrations -t 60"
          volumeMounts:    
            - name: dags-data
              mountPath: /opt/airflow/dags
              subPath: 
              readOnly: true
            - name: logs-data
              mountPath: /opt/airflow/logs
              subPath: 
      containers:
        - name: airflow-flower          
          image: apache/airflow:2.8.4-python3.9
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          resources:
            {}
          envFrom:            
            - secretRef:
                name: airflow-cluster-config-envs
          env:            
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgres-secret
                  key: postgres-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-cluster-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          ports:
            - name: flower
              containerPort: 5555
              protocol: TCP
          command:            
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec airflow celery flower"
          readinessProbe:
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 6
            exec:
              command:
                - "bash"
                - "-c"
                - "exec curl 'http://localhost:5555'"
          livenessProbe:
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 6
            exec:
              command:
                - "bash"
                - "-c"
                - "exec curl 'http://localhost:5555'"
          volumeMounts:            
            - name: dags-data
              mountPath: /opt/airflow/dags
              subPath: 
              readOnly: true
            - name: logs-data
              mountPath: /opt/airflow/logs
              subPath: 
      volumes:        
        - name: dags-data
          persistentVolumeClaim:
            claimName: airflow-cluster-dags
        - name: logs-data
          persistentVolumeClaim:
            claimName: airflow-cluster-logs
---
# Source: airflow/templates/pgbouncer/pgbouncer-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: airflow-cluster-pgbouncer
  labels:
    app: airflow
    component: pgbouncer
    chart: airflow-8.9.0
    release: airflow-cluster
    heritage: Helm
spec:
  replicas: 1
  strategy:
    rollingUpdate:
      ## multiple pgbouncer pods can safely run concurrently
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app: airflow
      component: pgbouncer
      release: airflow-cluster
  template:
    metadata:
      annotations:
        checksum/secret-config-envs: c01fcf854921a69c2cd9e437e9d5e73c4e69d5cbad62a0c53bd258bcd62996a3
        checksum/secret-pgbouncer: 45f5e065d95bf70f25c2b9b0d941d9308b7329c017286894cd635325890cfabb
        cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      labels:
        app: airflow
        component: pgbouncer
        release: airflow-cluster
    spec:
      restartPolicy: Always
      nodeSelector:
        {}
      topologySpreadConstraints:
        []
      affinity:
        {}
      tolerations:
        []
      securityContext:
        fsGroup: 0
      terminationGracePeriodSeconds: 120
      serviceAccountName: airflow-service-account
      containers:
        - name: pgbouncer
          image: ghcr.io/airflow-helm/pgbouncer:1.22.1-patch.0
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 1001
            runAsGroup: 1001
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 128Mi
          envFrom:            
            - secretRef:
                name: airflow-cluster-config-envs
          env:            
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgres-secret
                  key: postgres-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-cluster-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          ports:
            - name: pgbouncer
              containerPort: 6432
              protocol: TCP
          command:
            - "/usr/bin/dumb-init"
            ## rewrite SIGTERM as SIGINT, so pgbouncer does a safe shutdown
            - "--rewrite=15:2"
            - "--"
          args:
            - "/bin/sh"
            - "-c"
            ## we generate users.txt on startup, because DATABASE_PASSWORD is defined from a Secret,
            ## and we want to pickup the new values on container restart (possibly due to livenessProbe failure)
            - |-
              /home/pgbouncer/config/gen_self_signed_cert.sh && \
              /home/pgbouncer/config/gen_auth_file.sh && \
              exec pgbouncer /home/pgbouncer/config/pgbouncer.ini
          livenessProbe:
            initialDelaySeconds: 5
            periodSeconds: 30
            timeoutSeconds: 60
            failureThreshold: 3
            exec:
              command:
                - "/bin/sh"
                - "-c"
                ## this check is intended to fail when the DATABASE_PASSWORD secret is updated,
                ## which would cause `gen_auth_file.sh` to run again on container start
                - psql $(eval $DATABASE_PSQL_CMD) --tuples-only --command="SELECT 1;" | grep -q "1"
          startupProbe:
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 15
            failureThreshold: 30
            tcpSocket:
              port: 6432
          volumeMounts:
            - name: pgbouncer-config
              mountPath: /home/pgbouncer/config
              readOnly: true
      volumes:
        - name: pgbouncer-config
          secret:
            secretName: airflow-cluster-pgbouncer
            items:
              - key: gen_auth_file.sh
                path: gen_auth_file.sh
                mode: 0755
              - key: gen_self_signed_cert.sh
                path: gen_self_signed_cert.sh
                mode: 0755
              - key: pgbouncer.ini
                path: pgbouncer.ini
---
# Source: airflow/templates/scheduler/scheduler-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: airflow-cluster-scheduler
  labels:
    app: airflow
    component: scheduler
    chart: airflow-8.9.0
    release: airflow-cluster
    heritage: Helm
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      ## multiple schedulers can run concurrently (Airflow 2.0)
      maxSurge: 25%
      maxUnavailable: 0
  selector:
    matchLabels:
      app: airflow
      component: scheduler
      release: airflow-cluster
  template:
    metadata:
      annotations:
        checksum/secret-config-envs: c01fcf854921a69c2cd9e437e9d5e73c4e69d5cbad62a0c53bd258bcd62996a3
        checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
        cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      labels:
        app: airflow
        component: scheduler
        release: airflow-cluster
    spec:
      restartPolicy: Always
      nodeSelector:
        {}
      topologySpreadConstraints:
        []
      affinity:
        {}
      tolerations:
        []
      securityContext:
        fsGroup: 0
      serviceAccountName: airflow-service-account
      initContainers:        
        - name: check-db  
          image: apache/airflow:2.8.4-python3.9
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          resources:
            {}
          envFrom:    
            - secretRef:
                name: airflow-cluster-config-envs
          env:    
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgres-secret
                  key: postgres-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-cluster-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:    
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec timeout 60s airflow db check"
          volumeMounts:    
            - name: dags-data
              mountPath: /opt/airflow/dags
              subPath: 
              readOnly: true
            - name: logs-data
              mountPath: /opt/airflow/logs
              subPath:         
        - name: wait-for-db-migrations  
          image: apache/airflow:2.8.4-python3.9
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          resources:
            {}
          envFrom:    
            - secretRef:
                name: airflow-cluster-config-envs
          env:    
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgres-secret
                  key: postgres-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-cluster-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:    
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec airflow db check-migrations -t 60"
          volumeMounts:    
            - name: dags-data
              mountPath: /opt/airflow/dags
              subPath: 
              readOnly: true
            - name: logs-data
              mountPath: /opt/airflow/logs
              subPath: 
      containers:
        - name: airflow-scheduler          
          image: apache/airflow:2.8.4-python3.9
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          resources:
            limits:
              cpu: 1500m
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 512Mi
          envFrom:            
            - secretRef:
                name: airflow-cluster-config-envs
          env:            
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgres-secret
                  key: postgres-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-cluster-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:            
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec airflow scheduler -n -1"
          livenessProbe:
            initialDelaySeconds: 10
            periodSeconds: 30
            failureThreshold: 5
            timeoutSeconds: 60
            exec:
              command:                
                - "/usr/bin/dumb-init"
                - "--"
                - "/entrypoint"
                - "python"
                - "-Wignore"
                - "-c"
                - |
                  import os
                  import sys

                  # suppress logs triggered from importing airflow packages
                  os.environ["AIRFLOW__LOGGING__LOGGING_LEVEL"] = "ERROR"

                  # shared imports
                  try:
                      from airflow.jobs.job import Job
                  except ImportError:
                      # `BaseJob` was renamed to `Job` in airflow 2.6.0
                      from airflow.jobs.base_job import BaseJob as Job
                  from airflow.utils.db import create_session
                  from airflow.utils.net import get_hostname

                  with create_session() as session:
                      ########################
                      # heartbeat check
                      ########################
                      # ensure the SchedulerJob with most recent heartbeat for this `hostname` is alive
                      hostname = get_hostname()
                      scheduler_job = session \
                          .query(Job) \
                          .filter_by(job_type="SchedulerJob") \
                          .filter_by(hostname=hostname) \
                          .order_by(Job.latest_heartbeat.desc()) \
                          .limit(1) \
                          .first()
                      if (scheduler_job is not None) and scheduler_job.is_alive():
                          pass
                      else:
                          sys.exit(f"The SchedulerJob (id={scheduler_job.id}) for hostname '{hostname}' is not alive")
          volumeMounts:            
            - name: dags-data
              mountPath: /opt/airflow/dags
              subPath: 
              readOnly: true
            - name: logs-data
              mountPath: /opt/airflow/logs
              subPath: 
      volumes:        
        - name: dags-data
          persistentVolumeClaim:
            claimName: airflow-cluster-dags
        - name: logs-data
          persistentVolumeClaim:
            claimName: airflow-cluster-logs
---
# Source: airflow/templates/sync/sync-users-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: airflow-cluster-sync-users
  labels:
    app: airflow
    component: sync-users
    chart: airflow-8.9.0
    release: airflow-cluster
    heritage: Helm
spec:
  replicas: 1
  strategy:
    ## only 1 replica should run at a time
    type: Recreate
  selector:
    matchLabels:
      app: airflow
      component: sync-users
      release: airflow-cluster
  template:
    metadata:
      annotations:
        checksum/secret-config-envs: c01fcf854921a69c2cd9e437e9d5e73c4e69d5cbad62a0c53bd258bcd62996a3
        checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
        checksum/sync-users-script: 80d8c79eff1c0e5511869fbcb3b12d4b47096d5607b9e1262962adb970a45ff7
        cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      labels:
        app: airflow
        component: sync-users
        release: airflow-cluster
    spec:
      restartPolicy: Always
      nodeSelector:
        {}
      topologySpreadConstraints:
        []
      affinity:
        {}
      tolerations:
        []
      securityContext:
        fsGroup: 0
      serviceAccountName: airflow-service-account
      initContainers:        
        - name: check-db  
          image: apache/airflow:2.8.4-python3.9
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          resources:
            {}
          envFrom:    
            - secretRef:
                name: airflow-cluster-config-envs
          env:    
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgres-secret
                  key: postgres-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-cluster-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:    
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec timeout 60s airflow db check"
          volumeMounts:    
            - name: dags-data
              mountPath: /opt/airflow/dags
              subPath: 
              readOnly: true
            - name: logs-data
              mountPath: /opt/airflow/logs
              subPath:         
        - name: wait-for-db-migrations  
          image: apache/airflow:2.8.4-python3.9
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          resources:
            {}
          envFrom:    
            - secretRef:
                name: airflow-cluster-config-envs
          env:    
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgres-secret
                  key: postgres-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-cluster-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:    
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec airflow db check-migrations -t 60"
          volumeMounts:    
            - name: dags-data
              mountPath: /opt/airflow/dags
              subPath: 
              readOnly: true
            - name: logs-data
              mountPath: /opt/airflow/logs
              subPath: 
      containers:
        - name: sync-airflow-users          
          image: apache/airflow:2.8.4-python3.9
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          resources:
            {}
          envFrom:            
            - secretRef:
                name: airflow-cluster-config-envs
          env:            
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgres-secret
                  key: postgres-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-cluster-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:            
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "python"
            - "-u"
            - "/mnt/scripts/sync_users.py"
          volumeMounts:            
            - name: dags-data
              mountPath: /opt/airflow/dags
              subPath: 
              readOnly: true
            - name: logs-data
              mountPath: /opt/airflow/logs
              subPath: 
            - name: scripts
              mountPath: /mnt/scripts
              readOnly: true
      volumes:        
        - name: dags-data
          persistentVolumeClaim:
            claimName: airflow-cluster-dags
        - name: logs-data
          persistentVolumeClaim:
            claimName: airflow-cluster-logs
        - name: scripts
          secret:
            secretName: airflow-cluster-sync-users
---
# Source: airflow/templates/triggerer/triggerer-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: airflow-cluster-triggerer
  labels:
    app: airflow
    component: triggerer
    chart: airflow-8.9.0
    release: airflow-cluster
    heritage: Helm
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      ## multiple triggerer pods can safely run concurrently
      maxSurge: 25%
      maxUnavailable: 0
  selector:
    matchLabels:
      app: airflow
      component: triggerer
      release: airflow-cluster
  template:
    metadata:
      annotations:
        checksum/secret-config-envs: c01fcf854921a69c2cd9e437e9d5e73c4e69d5cbad62a0c53bd258bcd62996a3
        checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
        cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      labels:
        app: airflow
        component: triggerer
        release: airflow-cluster
    spec:
      restartPolicy: Always
      nodeSelector:
        {}
      topologySpreadConstraints:
        []
      affinity:
        {}
      tolerations:
        []
      serviceAccountName: airflow-service-account
      securityContext:
        fsGroup: 0
      initContainers:        
        - name: check-db  
          image: apache/airflow:2.8.4-python3.9
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          resources:
            {}
          envFrom:    
            - secretRef:
                name: airflow-cluster-config-envs
          env:    
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgres-secret
                  key: postgres-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-cluster-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:    
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec timeout 60s airflow db check"
          volumeMounts:    
            - name: dags-data
              mountPath: /opt/airflow/dags
              subPath: 
              readOnly: true
            - name: logs-data
              mountPath: /opt/airflow/logs
              subPath:         
        - name: wait-for-db-migrations  
          image: apache/airflow:2.8.4-python3.9
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          resources:
            {}
          envFrom:    
            - secretRef:
                name: airflow-cluster-config-envs
          env:    
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgres-secret
                  key: postgres-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-cluster-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:    
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec airflow db check-migrations -t 60"
          volumeMounts:    
            - name: dags-data
              mountPath: /opt/airflow/dags
              subPath: 
              readOnly: true
            - name: logs-data
              mountPath: /opt/airflow/logs
              subPath: 
      containers:
        - name: airflow-triggerer          
          image: apache/airflow:2.8.4-python3.9
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          resources:
            {}
          envFrom:            
            - secretRef:
                name: airflow-cluster-config-envs
          env:            
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgres-secret
                  key: postgres-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-cluster-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:            
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec airflow triggerer"
          livenessProbe:
            initialDelaySeconds: 10
            periodSeconds: 30
            timeoutSeconds: 60
            failureThreshold: 5
            exec:
              command:                
                - "/usr/bin/dumb-init"
                - "--"
                - "/entrypoint"
                - "python"
                - "-Wignore"
                - "-c"
                - |
                  import os
                  import sys

                  # suppress logs triggered from importing airflow packages
                  os.environ["AIRFLOW__LOGGING__LOGGING_LEVEL"] = "ERROR"

                  # shared imports
                  try:
                      from airflow.jobs.job import Job
                  except ImportError:
                      # `BaseJob` was renamed to `Job` in airflow 2.6.0
                      from airflow.jobs.base_job import BaseJob as Job
                  from airflow.utils.db import create_session
                  from airflow.utils.net import get_hostname

                  with create_session() as session:
                      # ensure the TriggererJob with most recent heartbeat for this `hostname` is alive
                      hostname = get_hostname()
                      triggerer_job = session \
                          .query(Job) \
                          .filter_by(job_type="TriggererJob") \
                          .filter_by(hostname=hostname) \
                          .order_by(Job.latest_heartbeat.desc()) \
                          .limit(1) \
                          .first()
                      if (triggerer_job is not None) and triggerer_job.is_alive():
                          pass
                      else:
                          sys.exit(f"The TriggererJob (id={triggerer_job.id}) for hostname '{hostname}' is not alive")
          volumeMounts:            
            - name: dags-data
              mountPath: /opt/airflow/dags
              subPath: 
              readOnly: true
            - name: logs-data
              mountPath: /opt/airflow/logs
              subPath: 
      volumes:        
        - name: dags-data
          persistentVolumeClaim:
            claimName: airflow-cluster-dags
        - name: logs-data
          persistentVolumeClaim:
            claimName: airflow-cluster-logs
---
# Source: airflow/templates/webserver/webserver-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: airflow-cluster-web
  labels:
    app: airflow
    component: web
    chart: airflow-8.9.0
    release: airflow-cluster
    heritage: Helm
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      ## multiple web pods can safely run concurrently
      maxSurge: 25%
      maxUnavailable: 0
  selector:
    matchLabels:
      app: airflow
      component: web
      release: airflow-cluster
  template:
    metadata:
      annotations:
        checksum/secret-config-envs: c01fcf854921a69c2cd9e437e9d5e73c4e69d5cbad62a0c53bd258bcd62996a3
        checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
        checksum/config-webserver-config: b5bb5cb5fa43b08d6dd4a3b240c81d016ae0b2e8f64fab7074200736b58e0e80
        cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      labels:
        app: airflow
        component: web
        release: airflow-cluster
    spec:
      restartPolicy: Always
      nodeSelector:
        {}
      topologySpreadConstraints:
        []
      affinity:
        {}
      tolerations:
        []
      serviceAccountName: airflow-service-account
      securityContext:
        fsGroup: 0
      initContainers:        
        - name: check-db  
          image: apache/airflow:2.8.4-python3.9
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          resources:
            {}
          envFrom:    
            - secretRef:
                name: airflow-cluster-config-envs
          env:    
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgres-secret
                  key: postgres-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-cluster-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:    
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec timeout 60s airflow db check"
          volumeMounts:    
            - name: dags-data
              mountPath: /opt/airflow/dags
              subPath: 
              readOnly: true
            - name: logs-data
              mountPath: /opt/airflow/logs
              subPath:         
        - name: wait-for-db-migrations  
          image: apache/airflow:2.8.4-python3.9
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          resources:
            {}
          envFrom:    
            - secretRef:
                name: airflow-cluster-config-envs
          env:    
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgres-secret
                  key: postgres-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-cluster-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:    
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec airflow db check-migrations -t 60"
          volumeMounts:    
            - name: dags-data
              mountPath: /opt/airflow/dags
              subPath: 
              readOnly: true
            - name: logs-data
              mountPath: /opt/airflow/logs
              subPath: 
      containers:
        - name: airflow-web          
          image: apache/airflow:2.8.4-python3.9
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          resources:
            {}
          ports:
            - name: web
              containerPort: 8080
              protocol: TCP
          envFrom:            
            - secretRef:
                name: airflow-cluster-config-envs
          env:            
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgres-secret
                  key: postgres-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-cluster-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:            
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec airflow webserver"
          livenessProbe:
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 6
            httpGet:
              scheme: HTTP
              path: /health
              port: web
          readinessProbe:
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 6
            httpGet:
              scheme: HTTP
              path: /health
              port: web
          volumeMounts:            
            - name: dags-data
              mountPath: /opt/airflow/dags
              subPath: 
              readOnly: true
            - name: logs-data
              mountPath: /opt/airflow/logs
              subPath: 
            - name: webserver-config
              mountPath: /opt/airflow/webserver_config.py
              subPath: webserver_config.py
              readOnly: true
      volumes:        
        - name: dags-data
          persistentVolumeClaim:
            claimName: airflow-cluster-dags
        - name: logs-data
          persistentVolumeClaim:
            claimName: airflow-cluster-logs
        - name: webserver-config
          secret:
            secretName: airflow-cluster-webserver-config
            defaultMode: 0644
---
# Source: airflow/charts/redis/templates/redis-master-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: airflow-cluster-redis-master
  labels:
    app: redis
    chart: redis-10.5.7
    release: airflow-cluster
    heritage: Helm
spec:
  selector:
    matchLabels:
      app: redis
      release: airflow-cluster
      role: master
  serviceName: airflow-cluster-redis-headless
  template:
    metadata:
      labels:
        app: redis
        chart: redis-10.5.7
        release: airflow-cluster
        role: master
      annotations:
        checksum/health: 30e74f55a0fad7c5f5d6c527f102f201d06a3e7ec5906f8073ed978185546322
        checksum/configmap: 17d6a4160844fcad4d46d32834868b1175ed83910c53eb2219db33ec06616b43
        checksum/secret: b37d105760bad5d2109b7aa97cc15ae401190d3488f14fe0335440fc55c6d083
        cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    spec:      
      securityContext:
        fsGroup: 1001
      serviceAccountName: "default"
      containers:
      - name: airflow-cluster-redis
        image: "docker.io/bitnami/redis:6.2.14-debian-12-r17"
        imagePullPolicy: "IfNotPresent"
        securityContext:
          runAsUser: 1001
        command:
        - /bin/bash
        - -c
        - |
          if [[ -n $REDIS_PASSWORD_FILE ]]; then
            password_aux=`cat ${REDIS_PASSWORD_FILE}`
            export REDIS_PASSWORD=$password_aux
          fi
          if [[ ! -f /opt/bitnami/redis/etc/master.conf ]];then
            cp /opt/bitnami/redis/mounted-etc/master.conf /opt/bitnami/redis/etc/master.conf
          fi
          if [[ ! -f /opt/bitnami/redis/etc/redis.conf ]];then
            cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf
          fi
          ARGS=("--port" "${REDIS_PORT}")
          ARGS+=("--requirepass" "${REDIS_PASSWORD}")
          ARGS+=("--masterauth" "${REDIS_PASSWORD}")
          ARGS+=("--include" "/opt/bitnami/redis/etc/redis.conf")
          ARGS+=("--include" "/opt/bitnami/redis/etc/master.conf")
          /run.sh ${ARGS[@]}
        env:
        - name: REDIS_REPLICATION_MODE
          value: master
        - name: REDIS_PASSWORD
          valueFrom:
            secretKeyRef:
              name: airflow-cluster-redis
              key: redis-password
        - name: REDIS_PORT
          value: "6379"
        ports:
        - name: redis
          containerPort: 6379
        livenessProbe:
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 5
          exec:
            command:
            - sh
            - -c
            - /health/ping_liveness_local.sh 5
        readinessProbe:
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 1
          successThreshold: 1
          failureThreshold: 5
          exec:
            command:
            - sh
            - -c
            - /health/ping_readiness_local.sh 5
        resources:
          {}
        volumeMounts:
        - name: health
          mountPath: /health
        - name: redis-data
          mountPath: /data
          subPath: 
        - name: config
          mountPath: /opt/bitnami/redis/mounted-etc
        - name: redis-tmp-conf
          mountPath: /opt/bitnami/redis/etc/
      volumes:
      - name: health
        configMap:
          name: airflow-cluster-redis-health
          defaultMode: 0755
      - name: config
        configMap:
          name: airflow-cluster-redis
      - name: "redis-data"
        emptyDir: {}
      - name: redis-tmp-conf
        emptyDir: {}
  updateStrategy:
    type: RollingUpdate
---
# Source: airflow/templates/worker/worker-statefulset.yaml
apiVersion: apps/v1
## StatefulSet gives workers consistent DNS names, allowing webserver access to log files
kind: StatefulSet
metadata:
  name: airflow-cluster-worker
  labels:
    app: airflow
    component: worker
    chart: airflow-8.9.0
    release: airflow-cluster
    heritage: Helm
spec:
  serviceName: "airflow-cluster-worker"
  replicas: 1
  updateStrategy:
    type: RollingUpdate
  ## we do not need to guarantee the order in which workers are scaled
  podManagementPolicy: Parallel
  selector:
    matchLabels:
      app: airflow
      component: worker
      release: airflow-cluster
  template:
    metadata:
      annotations:
        checksum/secret-config-envs: c01fcf854921a69c2cd9e437e9d5e73c4e69d5cbad62a0c53bd258bcd62996a3
        checksum/secret-local-settings: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
        cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      labels:
        app: airflow
        component: worker
        release: airflow-cluster
    spec:
      restartPolicy: Always
      terminationGracePeriodSeconds: 60
      serviceAccountName: airflow-service-account
      nodeSelector:
        {}
      topologySpreadConstraints:
        []
      affinity:
        {}
      tolerations:
        []
      securityContext:
        fsGroup: 0
      initContainers:        
        - name: check-db  
          image: apache/airflow:2.8.4-python3.9
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          resources:
            {}
          envFrom:    
            - secretRef:
                name: airflow-cluster-config-envs
          env:    
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgres-secret
                  key: postgres-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-cluster-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:    
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec timeout 60s airflow db check"
          volumeMounts:    
            - name: dags-data
              mountPath: /opt/airflow/dags
              subPath: 
              readOnly: true
            - name: logs-data
              mountPath: /opt/airflow/logs
              subPath:         
        - name: wait-for-db-migrations  
          image: apache/airflow:2.8.4-python3.9
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          resources:
            {}
          envFrom:    
            - secretRef:
                name: airflow-cluster-config-envs
          env:    
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgres-secret
                  key: postgres-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-cluster-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:    
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec airflow db check-migrations -t 60"
          volumeMounts:    
            - name: dags-data
              mountPath: /opt/airflow/dags
              subPath: 
              readOnly: true
            - name: logs-data
              mountPath: /opt/airflow/logs
              subPath: 
      containers:
        - name: airflow-worker          
          image: apache/airflow:2.8.4-python3.9
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 50000
            runAsGroup: 0
          resources:
            {}
          envFrom:            
            - secretRef:
                name: airflow-cluster-config-envs
          env:            
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-postgres-secret
                  key: postgres-password
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-cluster-redis
                  key: redis-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
            # have dumb-init only send signals to direct child process (needed for celery workers to warm shutdown)
            - name: DUMB_INIT_SETSID
              value: "0"
          livenessProbe:
            initialDelaySeconds: 10
            timeoutSeconds: 60
            failureThreshold: 5
            periodSeconds: 30
            exec:
              command:                
                - "/usr/bin/dumb-init"
                - "--"
                - "/entrypoint"
                - "python"
                - "-Wignore"
                - "-c"
                - |
                  import os
                  import sys
                  import subprocess
                  from celery import Celery
                  from celery.app.control import Inspect
                  from typing import List

                  def run_command(cmd: List[str]) -> str:
                      process = subprocess.Popen(cmd, stdout=subprocess.PIPE)
                      output, error = process.communicate()
                      if error is not None:
                          raise Exception(error)
                      else:
                          return output.decode(encoding="utf-8")

                  broker_url = run_command(["bash", "-c", "eval $AIRFLOW__CELERY__BROKER_URL_CMD"])
                  local_celery_host = f"celery@{os.environ['HOSTNAME']}"
                  app = Celery(broker=broker_url)

                  # ping the local celery worker to see if it's ok
                  i = Inspect(app=app, destination=[local_celery_host], timeout=5.0)
                  ping_responses = i.ping()
                  if local_celery_host not in ping_responses:
                      sys.exit(f"celery worker '{local_celery_host}' did not respond to ping")
          ports:
            - name: wlog
              containerPort: 8793
              protocol: TCP
          command:            
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec airflow celery worker"
          volumeMounts:            
            - name: dags-data
              mountPath: /opt/airflow/dags
              subPath: 
              readOnly: true
            - name: logs-data
              mountPath: /opt/airflow/logs
              subPath: 
      volumes:        
        - name: dags-data
          persistentVolumeClaim:
            claimName: airflow-cluster-dags
        - name: logs-data
          persistentVolumeClaim:
            claimName: airflow-cluster-logs
